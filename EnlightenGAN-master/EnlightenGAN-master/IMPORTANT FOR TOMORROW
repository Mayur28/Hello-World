Where does the perceptual loss fit in
I see where we are calculating the attention map but I dont see where are we using it.

Add print statements to the __getitems__ functions (especially the one that is used to create a dictionary containing the dataset). This is because the dictionary is only created in the __getitem__ function which does not seem to be explicitly called.

Look into the main functions (the arch. of the networks as well as how they forward and back propagate)

Filter out everything properly today!

Fix get_current_visuals function to save space!

In the long term, I may need to compile better datasets. I dont think training for the 2 roles simulataneously is a viable solution

My Experimentations:

Remove the redundant down sample in the generator
To upsample in the generator, try the transpose convolutional layer