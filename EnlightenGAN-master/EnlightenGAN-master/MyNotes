The default does account for self-attention and vgg
The single model is used
which_model_netG sid_unet_resize
which_model_netD no_norm_4
--patchD should be the local discriminator

Check what does the following do:
(I thought that the PatchGAn is standardized to 3 layers?)
--patchD
--patch_vgg
--patchD_3 5
--n_layers_D 5
--n_layers_patchD 4

My understanding of EGAN is slightly flawed. I thought EGAN primarily uses instance normalization which isnt the case.
Instead, it primarily uses batch norm but uses instance norm before the VGG for stability

The 0/1 seem to be boolean values
They use a patch size of 32 for the local discriminator
Skip connections are being used (But investigate how)

Check why no dropout? Seems to contradict MLM but outs is an attention-guided U-net generator
They are not using tanh for the output layer of the generator

The training uses the 'single model' . I need to train this model and analysis the results achieved
Just creating the model itself takes 5 minutes for some reason
__init__ isnt always necessary when working with classes. We only need it if we need every newly created instance of the class to
be set to some specifies values. We see that SingleModel does not how __init__ which is fine!

We are not using fcn!
There pretrained model that they provide is trained for 200 epochs
What is fineSize?
It seems more promising now that the configuration that they provide coincides with the provided pre-trained network.

I see that they include the instance normalization setting in the PerceptualLoss function

The type of Generator we using is: sid_unet_resize
Check what size images we are working with. The paper and dataset reflects 600x400 but the processing seems to account for 320x320

I see that they are using maxpooling, I should see if I can instead use the transpose_conv to down sample(This move is backed by Radford)

For our implementation, each block is: Conv->L_Relu->Batch->Conv->L_Relu->Batch (Followed by maxpooling)

Find out the following: Its usually Conv->Batch-> Relu but here its Conv-> Relu->Batch. Answer! MLM states that this is not an issue. Org. paper has it like normal but experimentation revealed that better perf.
is achieved if placed after the activation layer. If possible, try to test both!

Check where is the attention map( input to the network)  actually specified?
Check the resizing story and /or how the size correlates with the 600x400 input images?

For my own understanding, check how does the number of filters in a layer (and for BN) as we progress through the network. What effect does it have to the size( ie. what happens to the size of the rceptive field
when the number of filters increases.) THIS HAS BEEN A PERSISTENT CONFUSION FOR A WHILE!

Its a slight trick that we specify a normalization type when creating the generator but for our specified type, we dont need it. Same applies for the discriminator
As expected, they use a kernel size of for in the discriminator.
The discriminator is built algorithmically by defining a batch and using it as many times that is specified(n_layers_D)

They are not appending a sigmoid layer to the end of the discriminator?

I dont know what is there A and B notation for... Fake pool_b

Check where does the skip connections fit in?
There is a calculation associated with the self-attention map

They scale the images to 286 (loadSize) but why do they crop the image to 256. I should try this, scale to small,do whatever, then scale up

Check where is 'which_direction' used

I can used visdom (on Windows) using the linux shell without nohup!
visdom just displays the images, not the graphs, I should try the graphs
Get a brand new baby version of visdom to work, and take it from there! (the html must be extracted from drive)
fineSize is the official size of the images that we work with

I think that A represents the input and B is the output. THe first section of 'initialize' in Single model is the images that we are saving in drive

The vgg loss and the actual VGG networks get shipped to the GPU

They are not using sigmoid in the discriminator???

Look thoroughly into the generator/ discriminator structure and the GAN Loss

The losses used are deceiving --> Look into the criterion stuff in single_model
For some reason, we dont ever set the discriminators to trainable?

The dataloader loads the data into batches ( and pretty much handles all the data handling) by using the GPU. LOOK INTO THE DATALOADER STUFF THOROUGHLY!! To use the dataloader stuff, it is compulsory to implement the __getitem__ and __len__ magic functions

The 'data' training loop is a dictionary where each element is a tensor (with dimensions 16x3xsizexsize). These tensors represent A,B,input_A_gray and input_imgs. It also stores 'A_paths' which is the path to the training images accessed in that batch. Go deeper into how this dictionary is actually formed using the data loader.

Add print statements to the __getitems__ functions (especially the one that is used to create a dictionary containing the dataset). This is because the dictionary is only created in the __getitem__ function which does not seem to be explicitly called. I think it gets called automatically when we extract a batch in our training for-loop (through the dataloader)

Be careful where we need to set eval and test/train!

Remember, single model is the grand network which contains the generator and the discriminator. (we are working with an instance of 'single model'). That is why in 'optimize_parameters', we only calling forward() once which will propagate through the generator and the discriminator.  CHECK WHERE FDO WE DO THE ALTERNATING THING WHERE WE TRAIN WITH REAL AND FAKE SEPERATELY (AS RECOMMENDED BY RADFORD??). This could be impossible in our case because of the unsupervised setting that we are training in ( BUT RADFORD IS SPECIFIC TO THE UNSUPERVISED CASE ACCORDING TO THE TITLE?)

In the long term, I may need to compile better datasets. I dont think training for the 2 roles simulataneously is a viable solution

Where to we use the vgg loss? It doesnt seem to be incorporated with the GAN loss.

What is the folders A and B for? A is the dark and B is the light - which is the transition that we want.

Why do many of the classes accept a parameter? An example being the unaligned_dataset class in unaligned_dataset.py

The __getitems__ function in unaligned_dataset is called each iteration and is used to form the dictionary form of the batch with the A,B,input image and attention map!

I think they are flipping some of the images for data augmentation

I dont understand what are they doing for the input image???

I changed DataParallel because at the moment, I'm only using one GPU.This function basically chunks the input across all the GPU's (uncomment if Shun makes a plan)

Looking into the GANLoss class, we see that self.loss is actually set to MSEloss

When they say that they concatenate and reshaping the attention maps to the filter size, we actually do this in the decoder( get multiplied when we are upsampling)

The A's are the low-light images, B's are random normal light images

The generator output is not using tanh and the discriminator output is not using sigmoid

The patches are quite important

Confirm the story of switching the labels in backward_G!

For some magical reason, they are sure that real and fake images are in the range [-1,1] but there doesnt seems anything explicit to enforce this?

In vgg_preprocess, why are the colour channels reversed?


Verify the sequence that the weights are updated in!